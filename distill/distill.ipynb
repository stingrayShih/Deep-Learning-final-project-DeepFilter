{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE-CODE                \u001b[34mdeepseek_vl.egg-info\u001b[m\u001b[m/\n",
      "LICENSE-MODEL               gemini_explanations.jsonl\n",
      "Makefile                    gemini_explanations2.jsonl\n",
      "README.md                   \u001b[34mimages\u001b[m\u001b[m/\n",
      "cli_chat.py                 inference.py\n",
      "\u001b[34mdata\u001b[m\u001b[m/                       pyproject.toml\n",
      "\u001b[34mdeepseek_vl\u001b[m\u001b[m/                requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.nls             \u001b[34mDeepSeek-VL-main\u001b[m\u001b[m/ distill.ipynb     eval.py\n",
      "2.nls             dataset.py        distill.py        finetune.ipynb\n",
      "\u001b[34mDeepSeek-VL\u001b[m\u001b[m/      \u001b[34mdistill\u001b[m\u001b[m/          distill.zip       zeroshot.ipynb\n",
      "/Users/User/Desktop/Deep learning/deepseek/DeepSeek-VL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "%cd DeepSeek-VL/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (3.11.13)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.18-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting aiofiles\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (4.67.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: nest_asyncio in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (1.6.0)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.168.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from google-generativeai) (5.28.3)\n",
      "Collecting pydantic (from google-generativeai)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from google-generativeai) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from aiohttp) (1.18.3)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.7)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic->google-generativeai)\n",
      "  Downloading pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions (from google-generativeai)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->google-generativeai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.67.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Downloading aiohttp-3.11.18-cp311-cp311-macosx_11_0_arm64.whl (457 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading google_api_python_client-2.168.0-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.71.0-cp311-cp311-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, typing-extensions, tqdm, pyasn1, proto-plus, httplib2, grpcio, googleapis-common-protos, annotated-types, aiofiles, typing-inspection, rsa, pydantic-core, pyasn1-modules, grpcio-status, aiohttp, pydantic, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.0\n",
      "    Uninstalling tqdm-4.67.0:\n",
      "      Successfully uninstalled tqdm-4.67.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.67.1\n",
      "    Uninstalling grpcio-1.67.1:\n",
      "      Successfully uninstalled grpcio-1.67.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.13\n",
      "    Uninstalling aiohttp-3.11.13:\n",
      "      Successfully uninstalled aiohttp-3.11.13\n",
      "Successfully installed aiofiles-24.1.0 aiohttp-3.11.18 annotated-types-0.7.0 google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.168.0 google-auth-2.39.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.3 pydantic-core-2.33.1 rsa-4.9.1 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspection-0.4.0 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-generativeai aiohttp aiofiles tqdm nest_asyncio aiolimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== requirements ====\n",
    "# pip install --upgrade google-generativeai aiohttp aiofiles tqdm nest_asyncio\n",
    "\n",
    "import os, json, asyncio, aiofiles\n",
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import google.generativeai as genai\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "nest_asyncio.apply()                         # for Jupyter / Colab\n",
    "\n",
    "\n",
    "genai.configure(api_key=\"\")  # replace with your API key\n",
    "MODEL = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "SAFETY = [{\"category\": \"HATE_SPEECH\", \"threshold\": \"block_none\"}]\n",
    "\n",
    "PROMPT_TMPL = \"\"\"\\\n",
    "You are an expert content-moderation analyst.\n",
    "\n",
    "Image: <image>\n",
    "Meme text: {text}\n",
    "Known label: {label_str}\n",
    "\n",
    "Explain, in 2-3 sentences, why the meme is {label_str}.\n",
    "Write only the explanation; do not repeat the label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai  # pip install google-generativeai\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "# ==== Configuration ====\n",
    "GEMINI_API_KEY = \"\"  # <-- üî• Change this\n",
    "dev_path = \"data/dev.jsonl\"\n",
    "max_samples = 500  # Set to None for all data, or a number like 100 for quick testing\n",
    "\n",
    "# ==== Initialize Gemini ====\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "MODEL = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# ==== Load data ====\n",
    "print(\"Loading data...\")\n",
    "examples = []\n",
    "def load_jsonl(p): return [json.loads(l) for l in open(p, encoding=\"utf-8\")]\n",
    "\n",
    "examples  = load_jsonl(\"data/dev.jsonl\")\n",
    "\"\"\"ArithmeticError\n",
    "with open(dev_path, 'r') as f:\n",
    "    for line in f:\n",
    "        examples.append(json.loads(line.strip()))\n",
    "\"\"\"\n",
    "\n",
    "if max_samples:\n",
    "    examples = examples[:max_samples]\n",
    "print(examples[0])\n",
    "# ==== Evaluation ====\n",
    "prompt_prefix = (\n",
    "    \"Determine whether this meme is hateful or not. \"\n",
    "    \"First give a 1-2 sentence explanation, then on a new line write \"\n",
    "    \"FINAL ANSWER: YES or NO.\\n\"\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "groundtruth = []\n",
    "\n",
    "print(f\"Running on {len(examples)} samples...\")\n",
    "\n",
    "for ex in tqdm(examples):\n",
    "    image_path = ex[\"img\"]  # example: img/08291.png\n",
    "    text = ex[\"text\"]\n",
    "    label = ex[\"label\"]  # 1 = hateful (YES), 0 = non-hateful (NO)\n",
    "\n",
    "    # Prepare prompt\n",
    "    full_prompt = prompt_prefix + f\"Meme text: {text}\"\n",
    "\n",
    "    try:\n",
    "        # Read image bytes\n",
    "        with open(Path(\"data\") / image_path, \"rb\") as f:\n",
    "            image_bytes = f.read()\n",
    "\n",
    "        # Send to Gemini\n",
    "        response = MODEL.generate_content(\n",
    "            contents=[\n",
    "                {\"text\": full_prompt},\n",
    "                {\"mime_type\": \"image/png\", \"data\": image_bytes},\n",
    "            ]\n",
    "        )\n",
    "        output_text = response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error with {image_path}: {e}\")\n",
    "        predictions.append(-1)\n",
    "        groundtruth.append(label)\n",
    "        continue\n",
    "\n",
    "    # Parse YES/NO\n",
    "    #print(f\"Output: {output_text}\")\n",
    "    if \"YES\" in output_text:\n",
    "        predictions.append(1)\n",
    "    elif \"NO\" in output_text:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(-1)  # invalid output\n",
    "\n",
    "    groundtruth.append(label)\n",
    "\n",
    "# ==== Compute Accuracy ====\n",
    "correct = 0\n",
    "total = 0\n",
    "for pred, gt in zip(predictions, groundtruth):\n",
    "    if pred == -1:\n",
    "        continue\n",
    "    if pred == gt:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\n‚úÖ Gemini Rough Accuracy: {accuracy:.4f} ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8484 already finished, 16 left\n",
      "[BlockedPrompt] ID 50312 got blocked. Writing PROHIBITED_CONTENT.\n",
      "\n",
      "ID 50312 ‚Üí PROHIBITED_CONTENT ‚Ä¶\n",
      "[BlockedPrompt] ID 28507 got blocked. Writing PROHIBITED_CONTENT.\n",
      "\n",
      "ID 28507 ‚Üí PROHIBITED_CONTENT ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BlockedPrompt] ID 34897 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 98406 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 71834 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 47510 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 30256 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 59260 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 93502 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 13695 got blocked. Writing PROHIBITED_CONTENT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:02<00:00,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BlockedPrompt] ID 35740 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 13679 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 12793 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 26543 got blocked. Writing PROHIBITED_CONTENT.\n",
      "[BlockedPrompt] ID 52743 got blocked. Writing PROHIBITED_CONTENT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:03<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BlockedPrompt] ID 34295 got blocked. Writing PROHIBITED_CONTENT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json, asyncio, aiofiles, time\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from aiolimiter import AsyncLimiter\n",
    "from google.api_core import exceptions as gexc\n",
    "import google.generativeai as genai\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ local token bucket to respect 1 000 000 TPM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "TOKENS_PER_MIN, WIN = 1_000_000, 60\n",
    "recent, tok_sum = deque(), 0\n",
    "def token_wait(toks):\n",
    "    \"\"\"Block until <toks> fit into rolling 60-s window.\"\"\"\n",
    "    global tok_sum\n",
    "    now = time.monotonic()\n",
    "    while recent and now - recent[0][0] > WIN:\n",
    "        ts, v = recent.popleft(); tok_sum -= v\n",
    "    if tok_sum + toks > TOKENS_PER_MIN:\n",
    "        sleep = WIN - (now - recent[0][0]) if recent else WIN\n",
    "        print(f\"[TPM] sleeping {round(sleep,1)} s ‚Ä¶\")\n",
    "        time.sleep(sleep)\n",
    "        token_wait(toks); return\n",
    "    recent.append((now, toks)); tok_sum += toks\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ async request-limiter 15 RPM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "req_limiter = AsyncLimiter(50, 30)          # requests / minute\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def load_jsonl(p): return [json.loads(l) for l in open(p, encoding=\"utf-8\")]\n",
    "\n",
    "def already_done(out_path: Path) -> set[int]:\n",
    "    if not out_path.exists(): return set()\n",
    "    with out_path.open() as f:\n",
    "        return {json.loads(line)[\"id\"] for line in f}\n",
    "\n",
    "async def call_gemini(pil_img, prompt, retries=5):\n",
    "    for attempt in range(1, retries+1):\n",
    "        try:\n",
    "            # rate-limit requests\n",
    "            async with req_limiter:\n",
    "                resp = await asyncio.to_thread(\n",
    "                    MODEL.generate_content,\n",
    "                    [pil_img, prompt],\n",
    "                    stream=True,\n",
    "                    safety_settings=SAFETY,\n",
    "                )\n",
    "            break\n",
    "        except genai.types.BlockedPromptException as e:\n",
    "            print(f\"[BlockedPrompt] Skipping sample due to safety filters.\")\n",
    "            return \"PROHIBITED_CONTENT\"  # <-- safely skip this sample\n",
    "        \n",
    "        except gexc.ResourceExhausted as err:\n",
    "            # wait for quota reset\n",
    "            delay = getattr(err, \"retry_delay\", None)\n",
    "            delay = delay.seconds if delay else 40\n",
    "            print(f\"[429] sleeping {delay}s ‚Ä¶\")\n",
    "            await asyncio.sleep(delay)\n",
    "    else:\n",
    "        raise RuntimeError(\"Too many 429 retries\")\n",
    "\n",
    "    # collect streamed text + token count\n",
    "    text_parts = []\n",
    "    for chunk in resp:                                   # stream iterator\n",
    "        if chunk.candidates and chunk.candidates[0].content.parts:\n",
    "            text_parts.append(chunk.candidates[0].content.parts[0].text)\n",
    "\n",
    "    if not text_parts:                                   # finish_reason 8\n",
    "        print(f\"[warning] no text returned for ID {sample['id']}\")\n",
    "    token_wait(resp.usage_metadata.total_token_count)\n",
    "    return \" \".join(text_parts).strip()\n",
    "\n",
    "async def explain_one(sample, fout):\n",
    "    label_str = \"HATEFUL\" if sample[\"label\"] else \"NOT HATEFUL\"\n",
    "    prompt    = PROMPT_TMPL.format(text=sample[\"text\"], label_str=label_str)\n",
    "\n",
    "    try:\n",
    "        pil_img = Image.open(Path(\"data\") / sample[\"img\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[error] failed to open image {sample['img']}: {e}\")\n",
    "        reason = \"IMAGE_LOAD_FAILED\"\n",
    "    else:\n",
    "        try:\n",
    "            reason = await call_gemini(pil_img, prompt)\n",
    "        except genai.types.BlockedPromptException as e:\n",
    "            print(f\"[BlockedPrompt] ID {sample['id']} got blocked. Writing PROHIBITED_CONTENT.\")\n",
    "            reason = \"PROHIBITED_CONTENT\"\n",
    "        except Exception as e:\n",
    "            print(f\"[error] ID {sample['id']} failed due to unknown error: {e}\")\n",
    "            reason = \"CALL_FAILED\"\n",
    "\n",
    "    record = {**sample, \"gemini_reason\": reason}\n",
    "    await fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    await fout.flush()\n",
    "    return record\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main coroutine ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "async def main():\n",
    "    data      = load_jsonl(\"data/train.jsonl\")\n",
    "    out_path  = Path(\"gemini_explanations.jsonl\")\n",
    "    done_ids  = already_done(out_path)\n",
    "\n",
    "    # keep order but skip completed\n",
    "    pending   = [s for s in data if s[\"id\"] not in done_ids]\n",
    "    print(f\"{len(done_ids)} already finished, {len(pending)} left\")\n",
    "\n",
    "    async with aiofiles.open(out_path, \"a\") as fout:  # append-mode\n",
    "        # ‚îÄ‚îÄ sanity-check first two new items ‚îÄ‚îÄ\n",
    "        for samp in pending[:2]:\n",
    "            try:\n",
    "                rec = await explain_one(samp, fout)\n",
    "                print(\"\\nID\", rec[\"id\"], \"‚Üí\", rec[\"gemini_reason\"][:120], \"‚Ä¶\")\n",
    "            except Exception as e:\n",
    "                print(f\"[error] ID {samp['id']} failed during initial test: {e}\")\n",
    "\n",
    "        # ‚îÄ‚îÄ concurrent processing of remaining memes ‚îÄ‚îÄ\n",
    "        tasks = [explain_one(s, fout) for s in pending[2:]]\n",
    "        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n",
    "            try:\n",
    "                await coro\n",
    "            except Exception as e:\n",
    "                print(f\"[error] Task failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
