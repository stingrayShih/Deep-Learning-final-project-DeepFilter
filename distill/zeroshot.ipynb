{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/User/Desktop/Deep learning/deepseek/DeepSeek-VL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd DeepSeek-VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml_hw3/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:594: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Hugging Face / DeepSeek-VL\n",
    "from transformers import AutoModelForCausalLM\n",
    "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
    "from deepseek_vl.utils.io import load_pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    \"\"\"Load a .jsonl file into a list of dicts.\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 2) Load your dataset\n",
    "# ---------------------------------------------------\n",
    "train_data = load_jsonl(\"data/train.jsonl\")\n",
    "test_data = load_jsonl(\"data/test.jsonl\")\n",
    "dev_data = load_jsonl(\"data/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (0=non-hateful, 1=hateful):\n",
      "  Label 0: 5450 examples\n",
      "  Label 1: 3050 examples\n",
      "Label distribution (0=non-hateful, 1=hateful):\n",
      "  Label 1: 250 examples\n",
      "  Label 0: 250 examples\n",
      "\n",
      "No images have conflicting labels in train_data.\n"
     ]
    }
   ],
   "source": [
    "# 2) Distribution of labels\n",
    "from collections import Counter, defaultdict\n",
    "labels = [ex[\"label\"] for ex in train_data]\n",
    "label_dist = Counter(labels)\n",
    "print(\"Label distribution (0=non-hateful, 1=hateful):\")\n",
    "for lbl, count in label_dist.items():\n",
    "    print(f\"  Label {lbl}: {count} examples\")\n",
    "\n",
    "dev_labels = [ex[\"label\"] for ex in dev_data]\n",
    "dev_label_dist = Counter(dev_labels)\n",
    "print(\"Label distribution (0=non-hateful, 1=hateful):\")\n",
    "for lbl, count in dev_label_dist.items():\n",
    "    print(f\"  Label {lbl}: {count} examples\")\n",
    "# Example output:\n",
    "# Label 0: 3000 examples\n",
    "# Label 1: 2000 examples\n",
    "# 3) Check for same img path but different labels\n",
    "img_to_labels = defaultdict(set)\n",
    "for ex in train_data:\n",
    "    img_to_labels[ex[\"img\"]].add(ex[\"label\"])\n",
    "\n",
    "duplicates = [(img, list(labels)) for img, labels in img_to_labels.items()\n",
    "              if len(labels) > 1]\n",
    "\n",
    "if duplicates:\n",
    "    print(\"\\nImages with multiple labels in train_data:\")\n",
    "    for img, lbls in duplicates:\n",
    "        print(f\"  {img} => {lbls}\")\n",
    "else:\n",
    "    print(\"\\nNo images have conflicting labels in train_data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 57302, 'img': 'img/57302.png', 'label': 1, 'text': 'because they love you they want this not they need to mix breed to preserve their species'}, {'id': 2918, 'img': 'img/02918.png', 'label': 0, 'text': 'a islamic leader among his followers'}, {'id': 23401, 'img': 'img/23401.png', 'label': 0, 'text': \"i just realized we know we don't want him in the same bathroom with our wives\"}]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "few_shot_examples = train_data[:2]\n",
    "# Separate train_data by label\n",
    "non_hateful_data = [ex for ex in train_data if ex[\"label\"] == 0]\n",
    "hateful_data     = [ex for ex in train_data if ex[\"label\"] == 1]\n",
    "\n",
    "# 2) Pick 2 from label=0, 1 from label=1\n",
    "few_shot_non_hateful = random.sample(non_hateful_data, 2)\n",
    "few_shot_hateful     = random.sample(hateful_data, 1)\n",
    "\n",
    "# Combine them into a few-shot set (total 3 examples)\n",
    "few_shot_examples = few_shot_non_hateful + few_shot_hateful\n",
    "# Optionally shuffle them so the order isn't always the same\n",
    "random.shuffle(few_shot_examples)\n",
    "print(few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: sft_format, mask_prompt, ignore_id, image_tag, num_image_tokens, add_special_token. \n"
     ]
    }
   ],
   "source": [
    "model_path = \"deepseek-ai/deepseek-vl-1.3B-base\"  # or the chat variant\n",
    "vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).to(\"mps\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt(example):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": (\n",
    "                \"Classify the following meme as hateful or not hateful.\\n\\n\"\n",
    "                f\"Meme text: {example['text']}\\n\"\n",
    "                \"At the end of your answer, write:\\n\"\n",
    "                \"Final Answer: [HATEFUL or NOT HATEFUL]\\n\"\n",
    "                \"Provide no additional text.\\n\"\n",
    "            ),\n",
    "            \"images\": [f\"data/{example['img']}\"],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(example, few_shot_examples):\n",
    "    \"\"\"\n",
    "    Construct a conversation with a few labeled examples, then ask for the label of the new example.\n",
    "    \"\"\"\n",
    "    # Build the prefix from the few-shot examples\n",
    "    # We'll just do 2 examples here for brevity:\n",
    "    prompt_parts = []\n",
    "    for fs_ex in few_shot_examples:\n",
    "        label_text = \"HATEFUL\" if fs_ex[\"label\"] == 1 else \"NOT HATEFUL\"\n",
    "        prompt_parts.append(\n",
    "            f\"Meme: <image_placeholder>\\n\"\n",
    "            f\"Meme text: {fs_ex['text']}\\n\"\n",
    "            f\"Label: {label_text}\\n\\n\"\n",
    "        )\n",
    "    # Combine the few-shot context, then add the new sample\n",
    "    prompt_str = \"\".join(prompt_parts)\n",
    "    prompt_str += (\n",
    "        f\"Meme: <image_placeholder>\\n\"\n",
    "        f\"Meme text: {example['text']}\\n\"\n",
    "        \"Question: Is this meme hateful or not?\\n\"\n",
    "        \"Please answer 'HATEFUL' or 'NOT HATEFUL'.\"\n",
    "    )\n",
    "\n",
    "    # We have multiple images in the conversation: few-shot images, plus the new one\n",
    "    # We'll line them up in the same order we used in prompt_parts\n",
    "    images = []\n",
    "    for fs_ex in few_shot_examples:\n",
    "        images.append(os.path.join(\"data\", fs_ex[\"img\"]))\n",
    "    images.append(os.path.join(\"data\", example[\"img\"]))\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": prompt_str,\n",
    "            \"images\": images,\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "    return conversation\n",
    "\n",
    "def chain_of_thought_prompt(example):\n",
    "    \"\"\"\n",
    "    Construct a conversation that explicitly requests chain-of-thought reasoning,\n",
    "    then a final label.\n",
    "    \"\"\"\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": (\n",
    "                f\"Here is a meme: <image_placeholder>\\n\"\n",
    "                f\"Meme text: {example['text']}\\n\\n\"\n",
    "                \"Please reason step-by-step if it is hateful or not.\\n\"\n",
    "                \"Then provide the final answer as 'HATEFUL' or 'NOT HATEFUL'.\\n\"\n",
    "                \"Do not write anything else after your Final Answer.\\n\"\n",
    "                \"Chain of thought:\\n\"\n",
    "            ),\n",
    "            \"images\": [os.path.join(\"data\", example[\"img\"])],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label_from_output(output_text):\n",
    "    \"\"\"\n",
    "    Parse the model output to decide if it is 'HATEFUL' or 'NOT HATEFUL'.\n",
    "    Return 1 if 'HATEFUL', 0 if 'NOT HATEFUL'. If uncertain, default to 0.\n",
    "    \"\"\"\n",
    "    # Simple approach: search for these substrings\n",
    "    # Make them uppercase for easier matching\n",
    "    out_upper = output_text.upper()\n",
    "    #print(\"Response:\", out_upper)\n",
    "    if \"HATEFUL\" in out_upper and \"NOT HATEFUL\" not in out_upper:\n",
    "        return 1\n",
    "    elif \"NOT HATEFUL\" in out_upper:\n",
    "        return 0\n",
    "    else:\n",
    "        # No clear label found => default to not hateful\n",
    "        print(\"Warning: No clear label found in output.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference(conversation, processor, model):\n",
    "    \"\"\"\n",
    "    Given a conversation structure, run the DeepSeek-VL model \n",
    "    and return the raw text output.\n",
    "    \"\"\"\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepared = processor(\n",
    "        conversations=conversation,\n",
    "        images=pil_images,\n",
    "        force_batchify=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs_embeds = model.prepare_inputs_embeds(**prepared)\n",
    "        outputs = model.language_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=prepared.attention_mask,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "    decoded = processor.tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "Method: chain_of_thought\n",
      "  Accuracy: 0.0000\n",
      "  AUROC:    nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "for method, (acc, auroc) in results.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  AUROC:    {auroc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating few_shot: 100%|██████████| 500/500 [6:26:25<00:00, 46.37s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Method: few_shot\n",
      "  Accuracy: 0.5060\n",
      "  AUROC:    0.5060\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 3) Evaluate with each prompting method\n",
    "# ---------------------------------------------------\n",
    "methods = [ \"few_shot\"] #  \"zero_shot\", \"few_shot\", \"chain_of_thought\"\n",
    "results = {}\n",
    "cur_test_data = dev_data\n",
    "for method in methods:\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for example in tqdm(cur_test_data , desc=f\"Evaluating {method}\"): # test_data\n",
    "        if method == \"zero_shot\":\n",
    "            conversation = zero_shot_prompt(example)\n",
    "        elif method == \"few_shot\":\n",
    "            conversation = few_shot_prompt(example, few_shot_examples)\n",
    "        else:  # chain_of_thought\n",
    "            conversation = chain_of_thought_prompt(example)\n",
    "\n",
    "        output_text = run_inference(conversation, vl_chat_processor, vl_gpt)\n",
    "        pred_label = parse_label_from_output(output_text)\n",
    "        preds.append(pred_label)\n",
    "        labels.append(example[\"label\"])\n",
    "\n",
    "    # Compute accuracy, auroc\n",
    "    print(\"prediction:\", preds)\n",
    "    print(\"labels:\", labels)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # For AUROC, we need probas or continuous scores. We only have 0/1 from parse_label...\n",
    "    # In a real scenario, you might try to parse a \"confidence\" or do a second pass\n",
    "    # that requests a probability. But here we can do a \"binary\" interpretation for AUROC:\n",
    "    try:\n",
    "        auroc = roc_auc_score(labels, preds)\n",
    "    except ValueError:\n",
    "        # If there's only one class in the entire test set, roc_auc_score can fail\n",
    "        auroc = float(\"nan\")\n",
    "\n",
    "    results[method] = (acc, auroc)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) Print out the results\n",
    "# ---------------------------------------------------\n",
    "for method, (acc, auroc) in results.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  AUROC:    {auroc:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
